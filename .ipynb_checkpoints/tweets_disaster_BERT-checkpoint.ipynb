{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run data on GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('run data on GPU' )\n",
    "else:\n",
    "    print('run data on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"E:/Coding/Projects/NLP_tweets/Data file/\"\n",
    "data_train = pd.read_csv(file_path+'train.csv')\n",
    "data_test = pd.read_csv(file_path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALkAAABICAYAAACqY1TgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAABiUlEQVR4nO3YIU5DQRSG0TsNqkkdaASWYOgCWM3bB9tgGawDiUGBRNQQFJjBYEiaNk06eeXPOe5lRtybfGLyWu+9INli7gFgNJETT+TEEznxRE48kRPvbN+F1tpUVVNVVS2Wt7W8Gj3TfFZfc08w1up77gnGed9U//hs247aIf/J2+qm1/rxaHOdnLu3uScYK3m/6b76y+vWyD1XiCdy4omceCInnsiJJ3LiiZx4IieeyIkncuKJnHgiJ57IiSdy4omceCInnsiJJ3LiiZx4IieeyIkncuKJnHgiJ57IiSdy4omceCInnsiJJ3LiiZx4IieeyIkncuKJnHgiJ57IiSdy4omceCInnsiJJ3LiiZx4IieeyIkncuKJnHgiJ57IiSdy4omceCInXuu9777Q2lRV0+/ndVU9jx5qRudVtZl7iEGSd6uquuy9X2w72Bv5n8utPfXe10cb68Qk75e82z6eK8QTOfEOjfxhyBSnI3m/5N12OuhNDv+R5wrxRE48kRNP5MQTOfF+AGLnNE1Bxrr2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 216x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## visualize the labels\n",
    "\n",
    "current_palette = sns.color_palette('winter',3)\n",
    "sns.palplot(current_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.set_style('darkgrid')\n",
    "ax = sns.countplot(x='target',data=data_train,palette=current_palette)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax2 = ax.twinx()\n",
    "ax2.yaxis.tick_left()\n",
    "ax.yaxis.tick_right()\n",
    "ax.yaxis.set_label_position('right')\n",
    "ax2.yaxis.set_label_position('left')\n",
    "ax2.set_ylabel('Percentage %')\n",
    "\n",
    "# Fix the frequency range to 0-100\n",
    "ax2.set_ylim(0,100)\n",
    "ax.set_ylim(0,len(data_train['target']))\n",
    "\n",
    "for patch in ax.patches:\n",
    "    current_width = patch.get_width()\n",
    "    diff = current_width - 0.3\n",
    "\n",
    "    # change the bar width\n",
    "    patch.set_width(0.3)\n",
    "\n",
    "    # recenter the bar\n",
    "    patch.set_x(patch.get_x() + diff * .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the max length of sequences and batch size\n",
    "\n",
    "max_seq_len = 84\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pre-trained BERT tokenizer\n",
    "\n",
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data cleaning\n",
    "\n",
    "data_train = data_train[~(data_train.text.apply(lambda x : len(x)) > max_seq_len)]\n",
    "train_data = data_train[['text','target']]\n",
    "test_data = data_test[['id','text']]\n",
    "train_data.to_csv(\"train.tsv\", sep='\\t',index=False)\n",
    "test_data.to_csv(\"test.tsv\", sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text\n",
       "0         0                 Just happened a terrible car crash\n",
       "1         2  Heard about #earthquake is different cities, s...\n",
       "2         3  there is a forest fire at spot pond, geese are...\n",
       "3         9           Apocalypse lighting. #Spokane #wildfires\n",
       "4        11      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "...     ...                                                ...\n",
       "3258  10861  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n",
       "3259  10865  Storm in RI worse than last hurricane. My city...\n",
       "3260  10868  Green Line derailment in Chicago http://t.co/U...\n",
       "3261  10874  MEG issues Hazardous Weather Outlook (HWO) htt...\n",
       "3262  10875  #CityofCalgary has activated its Municipal Eme...\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Forest fire near La Ronge Sask. Canada, <class 'str'>\"\n",
      "['forest', 'fire', 'near', 'la', 'ron', '##ge', 'sas', '##k', '.', 'canada']\n",
      "[3224, 2543, 2379, 2474, 6902, 3351, 21871, 2243, 1012, 2710]\n"
     ]
    }
   ],
   "source": [
    "## convert the sequence into the tokens and get the index.\n",
    "\n",
    "text,_ = train_data.iloc[1].values\n",
    "pprint(f\"\"\"{text}, {type(text)}\"\"\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"[2 'Heard about #earthquake is different cities, stay safe everyone.'], \"\n",
      " \"<class 'str'>\")\n",
      "['[', '2', \"'\", 'heard', 'about', '#', 'earthquake', 'is', 'different', 'cities', ',', 'stay', 'safe', 'everyone', '.', \"'\", ']']\n",
      "[1031, 1016, 1005, 2657, 2055, 1001, 8372, 2003, 2367, 3655, 1010, 2994, 3647, 3071, 1012, 1005, 1033]\n"
     ]
    }
   ],
   "source": [
    "text_test = test_data.iloc[1].values\n",
    "text_test = np.array_str(text_test)\n",
    "pprint(f\"\"\"{text_test}, {type(text_test)}\"\"\")\n",
    "tokens_test = tokenizer.tokenize(text_test)\n",
    "print(tokens_test)\n",
    "ids_test = tokenizer.convert_tokens_to_ids(tokens_test)\n",
    "print(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use sklearn.model_selection to split the data into train and validation parts\n",
    "\n",
    "train_input, val_input = train_test_split(train_data, random_state=8787, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "convert the train and test data into the dataset which returns three tensors:\n",
    "- tokens_tensor：includes [CLS]\n",
    "- segments_tensor\n",
    "- label_tensor\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class NewDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mode, df, tokenizer):\n",
    "        \n",
    "        self.mode = mode\n",
    "        if  self.mode == 'test':\n",
    "            self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "            self.len = len(self.df)\n",
    "            self.tokenizer = tokenizer  \n",
    "        else:\n",
    "            self.df = df\n",
    "            self.len = len(self.df)\n",
    "            self.tokenizer = tokenizer  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.mode == \"test\":\n",
    "            text = self.df.iloc[idx].values\n",
    "            text = np.array_str(text)\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text, label = self.df.iloc[idx].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "                   \n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        word_pieces += tokens \n",
    "        length = len(word_pieces)\n",
    "                \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        segments_tensor = torch.tensor([0] * length, \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "## initialize the dataset\n",
    "\n",
    "trainset = NewDataset(None, train_input, tokenizer=tokenizer)\n",
    "valset = NewDataset(None, val_input, tokenizer=tokenizer)\n",
    "testset = NewDataset('test', df=None, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[text]\n",
      "sequence 1：They evacuated the mall. Again. ??\n",
      "label  ：1\n",
      "\n",
      "--------------------\n",
      "\n",
      "[tensors]\n",
      "tokens_tensor  ：tensor([  101,  2027, 13377,  1996,  6670,  1012,  2153,  1012,  1029,  1029])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "label_tensor   ：1\n",
      "\n",
      "--------------------\n",
      "\n",
      "[convert back]\n",
      "[CLS]theyevacuatedthemall.again.??\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check one sample and the output\n",
    "\n",
    "sample_idx = 1\n",
    "\n",
    "text, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "## convert the tokens_tensor back to the tokens\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "print(f\"\"\"[text]\n",
    "sequence 1：{text}\n",
    "label  ：{label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[convert back]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a DataLoader which can return mini-batch dataset\n",
    "and return four tensors BERT needs:\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    \n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    ## if test data has labels\n",
    "    \n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    ## zero padding\n",
    "    \n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    ## attention masks : set the positions which are not padding positions in the tokens_tensors to 1\n",
    "    ## BERT would only focus on these positions\n",
    "    \n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "                    tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "## initialize the DataLoader \n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, \n",
    "                         collate_fn=create_mini_batch)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([64, 37]) \n",
      "tensor([[  101,  1030,  2611,  ...,     0,     0,     0],\n",
      "        [  101,  2027, 13377,  ...,     0,     0,     0],\n",
      "        [  101,  1029,  2152,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 17542,  1996,  ...,     0,     0,     0],\n",
      "        [  101,  1030,  5572,  ...,     0,     0,     0],\n",
      "        [  101,  1045, 25430,  ...,     0,     0,     0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([64, 37])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([64, 37])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([64])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## demo\n",
    "\n",
    "data_demo = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data_demo\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "## fine-tuning BERT by add a classifier\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "       PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "\n",
    "## display modules\n",
    "\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    \n",
    "    if name == \"bert\":\n",
    "        \n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## define a function to get prediction \n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    \n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # parse the whole Dataloader\n",
    "        for data in dataloader:\n",
    "            \n",
    "            ## run data on the GPU\n",
    "            \n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            ## compute the accuracy\n",
    "            \n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            ## concat the prediction from every batch\n",
    "            \n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        \n",
    "        return predictions, acc\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "## run GPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train_loss: 11.029, train_acc: 0.872 val_loss: 4.276, val_acc: 0.903\n",
      "[epoch 2] train_loss: 7.982, train_acc: 0.914 val_loss: 2.885, val_acc: 0.950\n",
      "[epoch 3] train_loss: 5.990, train_acc: 0.954 val_loss: 1.984, val_acc: 0.979\n",
      "[epoch 4] train_loss: 4.640, train_acc: 0.958 val_loss: 0.910, val_acc: 0.980\n",
      "[epoch 5] train_loss: 4.210, train_acc: 0.972 val_loss: 1.204, val_acc: 0.983\n",
      "[epoch 6] train_loss: 3.308, train_acc: 0.984 val_loss: 0.749, val_acc: 0.987\n",
      "[epoch 7] train_loss: 2.366, train_acc: 0.980 val_loss: 1.027, val_acc: 0.983\n",
      "[epoch 8] train_loss: 2.291, train_acc: 0.981 val_loss: 0.680, val_acc: 0.990\n",
      "[epoch 9] train_loss: 1.567, train_acc: 0.991 val_loss: 0.521, val_acc: 0.990\n",
      "[epoch 10] train_loss: 1.361, train_acc: 0.988 val_loss: 0.435, val_acc: 0.993\n"
     ]
    }
   ],
   "source": [
    "## set optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "epochs = 10  \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    ## train mode\n",
    "    \n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## forward\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        ## backward\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        ## get the batch loss\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        \n",
    "    _, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    \n",
    "    ## evaluation mode\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    for data in valloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    _, val_acc = get_predictions(model, valloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] train_loss: %.3f, train_acc: %.3f' %(epoch + 1, training_loss, train_acc),\n",
    "          'val_loss: %.3f, val_acc: %.3f' %( val_loss, val_acc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(testset, batch_size=batch_size, collate_fn=create_mini_batch)\n",
    "\n",
    "## predict testset\n",
    "\n",
    "predictions = get_predictions(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1,  ..., 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target\n",
       "0          0\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1\n",
       "...      ...\n",
       "3258       1\n",
       "3259       1\n",
       "3260       1\n",
       "3261       1\n",
       "3262       1\n",
       "\n",
       "[3263 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create dataframe for kaggle submission\n",
    "\n",
    "df = pd.DataFrame({\"target\": predictions.tolist()})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.concat([testset.df.loc[:, [\"id\"]], df.loc[:, 'target']], axis=1)\n",
    "df_pred.to_csv('tweets_disaster_BERT.csv', index=False)\n",
    "df_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
